{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1Hm_MrugYBFkbNsHEbX58roWVxXla4-gz","authorship_tag":"ABX9TyM8T+UoegIqE19HfMlPEaSl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"90a18fkcetqX","executionInfo":{"status":"ok","timestamp":1692627847279,"user_tz":-480,"elapsed":12817,"user":{"displayName":"yang xy","userId":"06793310101143817236"}},"outputId":"261322a0-af89-4fb3-f63b-1109b192ca7b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"64XBuDxBcsQA","executionInfo":{"status":"ok","timestamp":1692628170141,"user_tz":-480,"elapsed":13589,"user":{"displayName":"yang xy","userId":"06793310101143817236"}},"outputId":"94e00245-c86e-43eb-e9f8-f1d150fb0bd6"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 6000/6000 [00:08<00:00, 688.87it/s]\n","100%|██████████| 2000/2000 [00:02<00:00, 781.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["split done.\n"]}],"source":["from transformers import AutoTokenizer  # 导入AutoTokenizer类，用于文本分词\n","import pandas as pd  # 导入pandas库，用于处理数据表格\n","import numpy as np  # 导入numpy库，用于科学计算\n","from tqdm import tqdm  # 导入tqdm库，用于显示进度条\n","import torch  # 导入torch库，用于深度学习任务\n","from torch.nn.utils.rnn import pad_sequence  # 导入pad_sequence函数，用于填充序列，保证向量中各序列维度的大小一样\n","\n","MAX_LENGTH = 128  # 定义最大序列长度为128\n","\n","def get_train(model_name, model_dict):\n","    model_index = model_dict[model_name]  # 获取模型索引\n","    train = pd.read_csv('./dataset/train.csv')  # 从CSV文件中读取训练数据\n","    train['content'] = train['title'] + train['author'] + train['abstract']  # 将标题、作者和摘要拼接为训练内容\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, max_length=MAX_LENGTH, cache_dir=f'./premodels/{model_name}_saved')  # 实例化分词器对象\n","    # 通过分词器对训练数据进行分词，并获取输入ID、注意力掩码和标记类型ID（这个可有可无）\n","    input_ids_list, attention_mask_list, token_type_ids_list = [], [], []\n","    y_train = []  # 存储训练数据的标签\n","\n","    for i in tqdm(range(len(train['content']))):  # 遍历训练数据\n","        sample = train['content'][i]  # 获取样本内容\n","        tokenized = tokenizer(sample, truncation='longest_first')  # 分词处理，使用最长优先方式截断\n","        input_ids, attention_mask = tokenized['input_ids'], tokenized['attention_mask']  # 获取输入ID和注意力掩码\n","        input_ids, attention_mask = torch.tensor(input_ids), torch.tensor(attention_mask)  # 转换为PyTorch张量\n","        try:\n","            token_type_ids = tokenized['token_type_ids']  # 获取标记类型ID\n","            token_type_ids = torch.tensor(token_type_ids)  # 转换为PyTorch张量\n","        except:\n","            token_type_ids = input_ids\n","        input_ids_list.append(input_ids)  # 将输入ID添加到列表中\n","        attention_mask_list.append(attention_mask)  # 将注意力掩码添加到列表中\n","        token_type_ids_list.append(token_type_ids)  # 将标记类型ID添加到列表中\n","        y_train.append(train['label'][i])  # 将训练数据的标签添加到列表中\n","    # 保存\n","    input_ids_tensor = pad_sequence(input_ids_list, batch_first=True, padding_value=0)  # 对输入ID进行填充，保证向量中各序列维度的大小一样，生成张量\n","    attention_mask_tensor = pad_sequence(attention_mask_list, batch_first=True, padding_value=0)  # 对注意力掩码进行填充，保证向量中各序列维度的大小一样，生成张量\n","    token_type_ids_tensor = pad_sequence(token_type_ids_list, batch_first=True, padding_value=0)  # 对标记类型ID进行填充，保证向量中各序列维度的大小一样，生成张量\n","    x_train = torch.stack([input_ids_tensor, attention_mask_tensor, token_type_ids_tensor], dim=1)  # 将输入张量堆叠为一个张量\n","    x_train = x_train.numpy()  # 转换为NumPy数组\n","    np.save(f'./models_input_files/x_train{model_index}.npy', x_train)  # 保存训练数据\n","    y_train = np.array(y_train)  # 将标签列表转换为NumPy数组\n","    np.save(f'./models_input_files/y_train{model_index}.npy', y_train)  # 保存标签数据\n","\n","def get_test(model_name, model_dict):\n","    model_index = model_dict[model_name]  # 获取模型索引\n","    test = pd.read_csv('./dataset/testB.csv')  # 从CSV文件中读取测试数据\n","    test['content'] = test['title'] + ' ' + test['author'] + ' ' + test['abstract']  # 将标题、作者和摘要拼接为测试内容\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, max_length=MAX_LENGTH,cache_dir=f'./premodels/{model_name}_saved')  # 实例化分词器对象\n","    # 通过分词器对测试数据进行分词，并获取输入ID、注意力掩码和标记类型ID（可有可无）\n","    input_ids_list, attention_mask_list, token_type_ids_list = [], [], []\n","\n","    for i in tqdm(range(len(test['content']))):  # 遍历测试数据\n","        sample = test['content'][i]  # 获取样本内容\n","        tokenized = tokenizer(sample, truncation='longest_first')  # 分词处理，使用最长优先方式截断\n","        input_ids, attention_mask = tokenized['input_ids'], tokenized['attention_mask']  # 获取输入ID和注意力掩码\n","        input_ids, attention_mask = torch.tensor(input_ids), torch.tensor(attention_mask)  # 转换为PyTorch张量\n","        try:\n","            token_type_ids = tokenized['token_type_ids']  # 获取标记类型ID\n","            token_type_ids = torch.tensor(token_type_ids)  # 转换为PyTorch张量\n","        except:\n","            token_type_ids = input_ids\n","        input_ids_list.append(input_ids)  # 将输入ID添加到列表中\n","        attention_mask_list.append(attention_mask)  # 将注意力掩码添加到列表中\n","        token_type_ids_list.append(token_type_ids)  # 将标记类型ID添加到列表中\n","\n","    # 保存\n","    input_ids_tensor = pad_sequence(input_ids_list, batch_first=True, padding_value=0)  # 对输入ID进行填充，保证向量中各序列维度的大小一样，生成张量\n","    attention_mask_tensor = pad_sequence(attention_mask_list, batch_first=True, padding_value=0)  # 对注意力掩码进行填充，保证向量中各序列维度的大小一样，生成张量\n","    token_type_ids_tensor = pad_sequence(token_type_ids_list, batch_first=True, padding_value=0)  # 对标记类型ID进行填充，保证向量中各序列维度的大小一样，生成张量\n","    x_test = torch.stack([input_ids_tensor, attention_mask_tensor, token_type_ids_tensor], dim=1)  # 将输入张量堆叠为一个张量\n","    x_test = x_test.numpy()  # 转换为NumPy数组\n","    np.save(f'./models_input_files/x_test{model_index}.npy', x_test)  # 保存测试数据\n","\n","def split_train(model_name, model_dict):\n","    # 处理样本内容\n","    model_index = model_dict[model_name]  # 获取模型索引\n","    train = np.load(f'./models_input_files/x_train{model_index}.npy')  # 加载训练数据\n","    state = np.random.get_state()  # 获取随机数状态，保证样本间的随机是可重复的\n","    np.random.shuffle(train)  # 随机打乱训练数据\n","    # 训练集:验证集 = 9 : 1\n","    val = train[int(train.shape[0] * 0.90):]  # 划分验证集\n","    train = train[:int(train.shape[0] * 0.90)]  # 划分训练集\n","    np.save(f'./models_input_files/x_train{model_index}.npy', train)  # 保存训练集\n","    np.save(f'./models_input_files/x_val{model_index}.npy', val)  # 保存验证集\n","    train = np.load(f'./models_input_files/y_train{model_index}.npy')  # 加载标签数据\n","\n","    # 处理样本标签\n","    np.random.set_state(state)  # 恢复随机数状态，让样本标签的随机可重复\n","    np.random.shuffle(train)  # 随机打乱标签数据\n","    # 训练集:验证集 = 9 : 1\n","    val = train[int(train.shape[0] * 0.90):]  # 划分验证集\n","    train = train[:int(train.shape[0] * 0.90)]  # 划分训练集\n","    np.save(f'./models_input_files/y_train{model_index}.npy', train)  # 保存训练集标签\n","    np.save(f'./models_input_files/y_val{model_index}.npy', val)  # 保存验证集标签\n","\n","    print('split done.')\n","\n","if __name__ == '__main__':\n","    model_dict = {'xlm-roberta-base':1, 'roberta-base':2, 'bert-base-uncased':3,\n","                  'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext':4, 'dmis-lab/biobert-base-cased-v1.2':5, 'marieke93/MiniLM-evidence-types':6,\n","                  'microsoft/MiniLM-L12-H384-uncased':7, 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext':8,'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract':9,\n","                  'microsoft/BiomedNLP-PubMedBERT-large-uncased-abstract':10}\n","    model_name = 'roberta-base'\n","    get_train(model_name, model_dict)\n","    get_test(model_name, model_dict)\n","    split_train(model_name, model_dict)"]},{"cell_type":"code","source":["# 导入需要的库\n","import numpy as np  # 导入numpy库，用于科学计算\n","import torch  # 导入torch库，用于深度学习任务\n","import torch.nn as nn  # 导入torch.nn模块，用于神经网络相关操作\n","from sklearn import metrics  # 导入sklearn库，用于评估指标计算\n","import os    # 导入os库，用于操作系统相关功能\n","import time  # 导入time库，用于时间相关操作\n","from transformers import AutoModel, AutoConfig  # 导入AutoModel和AutoConfig类，用于加载预训练模型\n","from tqdm import tqdm  # 导入tqdm库，用于显示进度条\n","\n","# 超参数类 - 可修改的所有超参数都在这里~\n","class opt:\n","    seed               = 42 # 随机种子\n","    batch_size         = 16 # 批处理大小\n","    set_epoch          = 5  # 训练轮数\n","    early_stop         = 5  # 提前停止epoch数\n","    learning_rate      = 1e-5 # 学习率\n","    weight_decay       = 2e-6 # 权重衰减,L2正则化\n","    device             = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 选择设备,GPU或CPU\n","    gpu_num            = 1 # GPU个数\n","    use_BCE            = False # 是否使用BCE损失函数\n","    models             = ['xlm-roberta-base', 'roberta-base', 'bert-base-uncased',\n","                          'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext', 'dmis-lab/biobert-base-cased-v1.2', 'marieke93/MiniLM-evidence-types',\n","                          'microsoft/MiniLM-L12-H384-uncased','cambridgeltl/SapBERT-from-PubMedBERT-fulltext', 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n","                          'microsoft/BiomedNLP-PubMedBERT-large-uncased-abstract'] # 模型名称列表\n","    model_index        = 2 # 根据上面选择使用的模型，这里填对应的模型索引\n","    model_name         = models[model_index-1] # 使用的模型名称\n","    continue_train     = False # 是否继续训练\n","    show_val           = False # 是否显示验证过程\n","\n","# 定义模型\n","class MODEL(nn.Module):\n","    def __init__(self, model_index):\n","        super(MODEL, self).__init__()\n","        # 若是第一次下载权重，则下载至同级目录的./premodels/内，以防占主目录的存储空间\n","        self.model = AutoModel.from_pretrained(opt.models[model_index-1], cache_dir='./premodels/'+opt.models[model_index-1]+'_saved', from_tf=False) # 加载预训练语言模型\n","        # 加载模型配置，可以直接获得模型最后一层的维度，而不需要手动修改\n","        config = AutoConfig.from_pretrained(opt.models[model_index-1], cache_dir='./premodels/'+opt.models[model_index-1]+'_saved') # 获取配置\n","        last_dim = config.hidden_size # 最后一层的维度\n","        if opt.use_BCE:out_size = 1 # 损失函数如果使用BCE,则输出大小为1\n","        else          :out_size = 2 # 否则则使用CE,输出大小为2\n","        feature_size = 128 # 设置特征的维度大小\n","        self.fc1 = nn.Linear(last_dim, feature_size) # 全连接层1\n","        self.fc2 = nn.Linear(last_dim, feature_size) # 全连接层2\n","        self.classifier = nn.Linear(feature_size, out_size) # 分类器\n","        self.dropout = nn.Dropout(0.3) # Dropout层\n","\n","\n","    def forward(self, x):\n","        input_ids, attention_mask, token_type_ids = x[:,0],x[:,1],x[:,2] # 获取输入\n","        x = self.model(input_ids, attention_mask) # 通过模型\n","\n","        all_token     = x[0] # 全部序列分词的表征向量\n","        pooled_output = x[1] # [CLS]的表征向量+一个全连接层+Tanh激活函数\n","\n","        feature1 = all_token.mean(dim=1) # 对全部序列分词的表征向量取均值\n","        feature1 = self.fc1(feature1)    # 再输入进全连接层，得到feature1\n","        feature2 = pooled_output      # [CLS]的表征向量+一个全连接层+Tanh激活函数\n","        feature2 = self.fc2(feature2) # 再输入进全连接层，得到feature2\n","        feature  = 0.5*feature1 + 0.5*feature2 # 加权融合特征\n","        feature  = self.dropout(feature) # Dropout\n","\n","        x  = self.classifier(feature) # 分类\n","        return x\n","\n","# 数据加载\n","def load_data():\n","    train_data_path     = f'models_input_files/x_train{model_index}.npy' # 训练集输入路径\n","    train_label_path    = f'models_input_files/y_train{model_index}.npy' # 训练集标签路径\n","    val_data_path       = f'models_input_files/x_val{model_index}.npy'   # 验证集输入路径\n","    val_label_path      = f'models_input_files/y_val{model_index}.npy'   # 验证集标签路径\n","    test_data_path      = f'models_input_files/x_test{model_index}.npy'  # 测试集输入路径\n","\n","    train_data          = torch.tensor(np.load(train_data_path  , allow_pickle=True).tolist()) # 载入训练集数据\n","    train_label         = torch.tensor(np.load(train_label_path  , allow_pickle=True).tolist()).long() # 载入训练集标签\n","    val_data            = torch.tensor(np.load(val_data_path  , allow_pickle=True).tolist()) # 载入验证集数据\n","    val_label           = torch.tensor(np.load(val_label_path  , allow_pickle=True).tolist()).long() # 载入验证集标签\n","    test_data           = torch.tensor(np.load(test_data_path  , allow_pickle=True).tolist()) # 载入测试集数据\n","\n","    train_dataset       = torch.utils.data.TensorDataset(train_data  , train_label) # 构造训练集Dataset\n","    val_dataset         = torch.utils.data.TensorDataset(val_data  , val_label) # 构造验证集Dataset\n","    test_dataset        = torch.utils.data.TensorDataset(test_data) # 构造测试集Dataset\n","\n","    return train_dataset, val_dataset, test_dataset # 返回数据集\n","\n","# 模型预训练\n","def model_pretrain(model_index, train_loader, val_loader):\n","    # 超参数设置\n","    set_epoch          = opt.set_epoch  # 训练轮数\n","    early_stop         = opt.early_stop # 提前停止epoch数\n","    learning_rate      = opt.learning_rate # 学习率\n","    weight_decay       = opt.weight_decay  # 权重衰减\n","    device             = opt.device  # 设备\n","    gpu_num            = opt.gpu_num # GPU个数\n","    continue_train     = opt.continue_train # 是否继续训练\n","    model_save_dir     = 'checkpoints' # 模型保存路径\n","\n","    # 是否要继续训练，若是，则加载模型进行训练；若否，则跳过训练，直接对测试集进行推理\n","    if not continue_train:\n","        # 判断最佳模型是否已经存在,若存在则直接读取,若不存在则进行预训练\n","        if os.path.exists(f'checkpoints/best_model{model_index}.pth'):\n","            best_model = MODEL(model_index)\n","            best_model.load_state_dict(torch.load(f'checkpoints/best_model{model_index}.pth')) # 加载模型\n","            return best_model\n","        else:\n","            pass\n","\n","\n","    # 模型初始化\n","    model = MODEL(model_index).to(device)\n","    if continue_train:\n","        model.load_state_dict(torch.load(f'checkpoints/best_model{model_index}.pth')) # 继续训练加载模型\n","\n","    # 优化器初始化\n","    if device    != 'cpu' and gpu_num > 1:  # 多张显卡\n","        optimizer = torch.optim.AdamW(model.module.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","        optimizer = torch.nn.DataParallel(optimizer, device_ids=list(range(gpu_num))) # 多GPU\n","    else: # 单张显卡\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # 单GPU\n","\n","    # 损失函数初始化\n","    if opt.use_BCE:\n","        loss_func = nn.BCEWithLogitsLoss() # BCE损失\n","    else:\n","        loss_func = nn.CrossEntropyLoss() # 交叉熵损失（CE）\n","\n","    # 模型训练\n","    best_epoch         = 0 # 最佳epoch\n","    best_train_loss    = 100000 # 最佳训练损失\n","    train_acc_list     = [] # 训练准确率列表\n","    train_loss_list    = [] # 训练损失列表\n","    val_acc_list       = [] # 验证准确率列表\n","    val_loss_list      = [] # 验证损失列表\n","    start_time         = time.time() # 训练开始时间\n","\n","    for epoch in range(set_epoch): # 轮数\n","        model.train() # 模型切换到训练模式\n","        train_loss = 0 # 训练损失\n","        train_acc = 0 # 训练准确率\n","        for x, y in tqdm(train_loader): # 遍历训练集\n","            # 训练前先将数据放到GPU上\n","            x        = x.to(device)\n","            y        = y.to(device)\n","            outputs  = model(x) # 前向传播\n","\n","            if opt.use_BCE: # BCE损失\n","                loss = loss_func(outputs, y.float().unsqueeze(1))\n","            else: # 交叉熵损失\n","                loss = loss_func(outputs, y)\n","            train_loss += loss.item() # 累加训练损失\n","            optimizer.zero_grad() # 清空梯度\n","            loss.backward() # 反向传播\n","\n","            if device != 'cpu' and gpu_num > 1: # 多GPU更新\n","                optimizer.module.step()\n","            else:\n","                optimizer.step() # 单GPU更新\n","\n","            if not opt.use_BCE: # 非BCE损失\n","                _, predicted = torch.max(outputs.data, 1) # 预测结果\n","            else:\n","                predicted = (outputs > 0.5).int() # 预测结果\n","                predicted = predicted.squeeze(1)\n","            train_acc   += (predicted == y).sum().item() # 计算训练准确率\n","\n","        average_mode = 'binary'\n","        train_f1     = metrics.f1_score(y.cpu(), predicted.cpu(), average=average_mode) # 计算F1\n","        train_pre    = metrics.precision_score(y.cpu(), predicted.cpu(), average=average_mode) # 计算精确率\n","        train_recall = metrics.recall_score(y.cpu(), predicted.cpu(), average=average_mode) # 计算召回率\n","\n","\n","        train_loss /= len(train_loader) # 平均所有步数的训练损失作为一个epoch的训练损失\n","        train_acc  /= len(train_loader.dataset) # 平均所有步数训练准确率作为一个epoch的准确率\n","        train_acc_list.append(train_acc)   # 添加训练准确率\n","        train_loss_list.append(train_loss) # 添加训练损失\n","\n","        print('-'*50)\n","        print('Epoch [{}/{}]\\n Train Loss: {:.4f}, Train Acc: {:.4f}'.format(epoch + 1, set_epoch, train_loss, train_acc))\n","        print('Train-f1: {:.4f}, Train-precision: {:.4f} Train-recall: {:.4f}'.format(train_f1, train_pre, train_recall))\n","\n","        if opt.show_val: # 显示验证过程\n","            # 验证\n","            model.eval() # 模型切换到评估模式\n","            val_loss = 0 # 验证损失\n","            val_acc = 0 # 验证准确率\n","\n","            for x, y in tqdm(val_loader): # 遍历验证集\n","                # 训练前先将数据放到GPU上\n","                x = x.to(device)\n","                y = y.to(device)\n","                outputs = model(x) # 前向传播\n","                if opt.use_BCE: # BCE损失\n","                    loss = loss_func(outputs, y.float().unsqueeze(1))\n","                else: # 交叉熵损失\n","                    loss = loss_func(outputs, y)\n","\n","                val_loss += loss.item() # 累加验证损失\n","                if not opt.use_BCE: # 非BCE损失\n","                    _, predicted = torch.max(outputs.data, 1)\n","                else:\n","                    predicted = (outputs > 0.5).int() # 预测结果\n","                    predicted = predicted.squeeze(1)\n","                val_acc += (predicted == y).sum().item() # 计算验证准确率\n","\n","            val_f1     = metrics.f1_score(y.cpu(), predicted.cpu(), average=average_mode) # 计算F1\n","            val_pre    = metrics.precision_score(y.cpu(), predicted.cpu(), average=average_mode) # 计算精确率\n","            val_recall = metrics.recall_score(y.cpu(), predicted.cpu(), average=average_mode) # 计算召回率\n","\n","            val_loss /= len(val_loader) # 平均验证损失\n","            val_acc /= len(val_loader.dataset) # 平均验证准确率\n","            val_acc_list.append(val_acc)   # 添加验证准确率\n","            val_loss_list.append(val_loss) # 添加验证损失\n","            print('\\nVal Loss: {:.4f}, Val Acc: {:.4f}'.format(val_loss, val_acc))\n","            print('Val-f1: {:.4f}, Val-precision: {:.4f} Val-recall: {:.4f}'.format(val_f1, val_pre, val_recall))\n","\n","        if train_loss < best_train_loss: # 更新最佳训练损失\n","            best_train_loss = train_loss\n","            best_epoch = epoch + 1\n","            if device == 'cuda' and gpu_num > 1: # 多GPU保存模型\n","                torch.save(model.module.state_dict(), f'{model_save_dir}/best_model{model_index}.pth')\n","            else:\n","                torch.save(model.state_dict(), f'{model_save_dir}/best_model{model_index}.pth') # 单GPU保存模型\n","\n","        # 提前停止判断\n","        if epoch+1 - best_epoch == early_stop:\n","            print(f'{early_stop} epochs later, the loss of the validation set no longer continues to decrease, so the training is stopped early.')\n","            end_time = time.time()\n","            print(f'Total time is {end_time - start_time}s.')\n","            break\n","\n","    best_model = MODEL(model_index) # 初始化最佳模型\n","    best_model.load_state_dict(torch.load(f'checkpoints/best_model{model_index}.pth')) # 加载模型参数\n","    return best_model # 返回最佳模型\n","\n","# 模型推理\n","def model_predict(model, model_index, test_loader):\n","    device = 'cuda'\n","    model.to(device) # 模型到GPU\n","    model.eval()  # 切换到评估模式\n","\n","    test_outputs = None\n","    with torch.no_grad():  # 禁用梯度计算\n","        for i, data in enumerate(tqdm(test_loader)):\n","            data = data[0].to(device) # 测试数据到GPU\n","            outputs = model(data) # 前向传播\n","            if i == 0:\n","                test_outputs = outputs # 第一个batch直接赋值\n","            else:\n","                test_outputs = torch.cat([test_outputs, outputs], dim=0) # 其余batch拼接\n","\n","            del data, outputs  # 释放不再需要的Tensor\n","\n","    # 保存预测结果\n","    if not opt.use_BCE:\n","        test_outputs = torch.softmax(test_outputs, dim=1) # 转换为概率\n","    torch.save(test_outputs, f'./models_prediction/{model_index}_prob.pth') # 保存概率\n","\n","def run(model_index):\n","    # 固定随机种子\n","    seed = opt.seed\n","    torch.seed = seed\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","    train_dataset, val_dataset, test_dataset = load_data() # 加载数据集\n","    # 打印数据集信息\n","    print('-数据集信息:')\n","    print(f'-训练集样本数:{len(train_dataset)},测试集样本数:{len(test_dataset)}')\n","    train_labels = len(set(train_dataset.tensors[1].numpy()))\n","    # 查看训练样本类别均衡状况\n","    print(f'-训练集的标签种类个数为:{train_labels}')\n","    numbers = [0] * train_labels\n","    for i in train_dataset.tensors[1].numpy():\n","        numbers[i] += 1\n","    print(f'-训练集各种类样本的个数:')\n","    for i in range(train_labels):\n","        print(f'-{i}的样本个数为:{numbers[i]}')\n","\n","    batch_size   = opt.batch_size # 批处理大小\n","    # 构建DataLoader\n","    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader   = torch.utils.data.DataLoader(dataset=val_dataset,   batch_size=batch_size, shuffle=True)\n","    test_loader  = torch.utils.data.DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False)\n","\n","    best_model   = model_pretrain(model_index, train_loader, val_loader)\n","\n","    # 使用验证集评估模型\n","    model_predict(best_model, model_index, test_loader) # 模型推理\n","\n","if __name__ == '__main__':\n","    model_index = opt.model_index # 获取模型索引\n","    run(model_index) # 运行程序"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":532},"id":"EeE4JSVggOE8","executionInfo":{"status":"error","timestamp":1692632066009,"user_tz":-480,"elapsed":27335,"user":{"displayName":"yang xy","userId":"06793310101143817236"}},"outputId":"20f6c089-8a32-429c-b237-acbe7881125c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["-数据集信息:\n","-训练集样本数:5400,测试集样本数:2000\n","-训练集的标签种类个数为:2\n","-训练集各种类样本的个数:\n","-0的样本个数为:2757\n","-1的样本个数为:2643\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  4%|▍         | 14/338 [00:22<08:37,  1.60s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-5bfc3a47c0b3>\u001b[0m in \u001b[0;36m<cell line: 293>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0mmodel_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_index\u001b[0m \u001b[0;31m# 获取模型索引\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 运行程序\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-5bfc3a47c0b3>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model_index)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mtest_loader\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     \u001b[0mbest_model\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mmodel_pretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;31m# 使用验证集评估模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-5bfc3a47c0b3>\u001b[0m in \u001b[0;36mmodel_pretrain\u001b[0;34m(model_index, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 预测结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtrain_acc\u001b[0m   \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 计算训练准确率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0maverage_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["\n","\n","import os\n","os.chdir('/content')\n","os.listdir('/content')\n","train_dataset, val_dataset, test_dataset = load_data()\n","test_loader  = torch.utils.data.DataLoader(dataset=test_dataset,  batch_size=128, shuffle=False)"],"metadata":{"id":"LagNr-yMpE3W","executionInfo":{"status":"ok","timestamp":1692632449503,"user_tz":-480,"elapsed":5222,"user":{"displayName":"yang xy","userId":"06793310101143817236"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["if os.path.exists(f'/content/drive/MyDrive/Colab Notebooks/best_model2.pth'):\n","            best_model = MODEL(model_index)\n","            best_model.load_state_dict(torch.load(f'/content/drive/MyDrive/Colab Notebooks/best_model2.pth')) # 加载模型\n","model = best_model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5d-6obcCrUqi","executionInfo":{"status":"ok","timestamp":1692632645924,"user_tz":-480,"elapsed":5620,"user":{"displayName":"yang xy","userId":"06793310101143817236"}},"outputId":"5f924773-3e5b-490c-d5c2-020e4130eab9"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["device = 'cuda'\n","model.to(device) # 模型到GPU\n","model.eval()  # 切换到评估模式\n","\n","test_outputs = None\n","with torch.no_grad():  # 禁用梯度计算\n","  for i, data in enumerate(tqdm(test_loader)):\n","    data = data[0].to(device) # 测试数据到GPU\n","    outputs = model(data) # 前向传播\n","    if i == 0:\n","      test_outputs = outputs # 第一个batch直接赋值\n","    else:\n","      test_outputs = torch.cat([test_outputs, outputs], dim=0)# 其余batch拼接\n","    del data, outputs  # 释放不再需要的Tensor\n","\n","    # 保存预测结果\n","    if not opt.use_BCE:\n","        test_outputs = torch.softmax(test_outputs, dim=1) # 转换为概率\n","  test_outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ehJhSURSv4Gt","executionInfo":{"status":"ok","timestamp":1692632718909,"user_tz":-480,"elapsed":68155,"user":{"displayName":"yang xy","userId":"06793310101143817236"}},"outputId":"0fd60d3b-8a5e-416f-bae2-c70347c8ab49"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [01:07<00:00,  4.20s/it]\n"]}]},{"cell_type":"code","source":["test_outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OEwK8OcMxbns","executionInfo":{"status":"ok","timestamp":1692632771561,"user_tz":-480,"elapsed":2,"user":{"displayName":"yang xy","userId":"06793310101143817236"}},"outputId":"05a7a514-379e-461a-97f4-7805581f5fef"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[4.9999e-01, 5.0001e-01],\n","        [4.9999e-01, 5.0001e-01],\n","        [5.0001e-01, 4.9999e-01],\n","        ...,\n","        [8.8911e-05, 9.9991e-01],\n","        [8.1559e-05, 9.9992e-01],\n","        [8.2474e-05, 9.9992e-01]], device='cuda:0')"]},"metadata":{},"execution_count":27}]}]}